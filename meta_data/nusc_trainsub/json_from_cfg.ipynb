{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vision_base.utils.builder import build\n",
    "from vision_base.utils.utils import cfg_from_file\n",
    "from monodepth.data.datasets.nuscene_dataset import cam_relative_pose_nusc\n",
    "\n",
    "BASEPATH= \"/home/yxliu/multi_cam/monodepth\" #Change this\n",
    "NUSCPATH = \"/data/nuscene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "import os\n",
    "train_dataset = edict(\n",
    "    name = \"monodepth.data.datasets.nuscene_dataset.NusceneDepthMonoDataset\",\n",
    "    frame_idxs = [0, 1, -1],\n",
    "    is_motion_mask = False,\n",
    "    is_precompute_flow = False,\n",
    "    is_filter_static = True,\n",
    "    nuscenes_dir = NUSCPATH,\n",
    "    split_file = os.path.join(BASEPATH, 'meta_data', 'nusc_trainsub', 'nusc_train.txt'),   \n",
    "    channels = ['CAM_FRONT'],\n",
    "    augmentation = edict(\n",
    "    name='vision_base.utils.builder.Sequential',\n",
    "    cfg_list=[\n",
    "        edict(name='vision_base.data.augmentations.augmentations.ConvertToFloat'),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.Resize', size=(288, 384), preserve_aspect_ratio=True, force_pad=True),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.Normalize', mean=np.array([0.485, 0.456, 0.406]), stds=np.array([0.229, 0.224, 0.225]),),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.ConvertToTensor'),\n",
    "    ],\n",
    "    image_keys=[('image', 0)], \n",
    "    calib_keys=['P2']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': 'lib.data.datasets.nuscene_dataset.NusceneDepthMonoDataset', 'frame_idxs': [0, 1, -1], 'is_motion_mask': False, 'is_precompute_flow': False, 'is_filter_static': True, 'nuscenes_dir': '/data/nuscene', 'split_file': '/home/yxliu/multi_cam/monodepth/meta_data/nusc_trainsub/nusc_train.txt', 'channels': ['CAM_FRONT'], 'augmentation': {'name': 'lib.utils.builder.Sequential', 'cfg_list': [{'name': 'lib.data.augmentations.augmentations.ConvertToFloat'}, {'name': 'lib.data.augmentations.augmentations.Resize', 'size': [288, 384], 'preserve_aspect_ratio': True, 'force_pad': True}, {'name': 'lib.data.augmentations.augmentations.Normalize', 'mean': array([0.485, 0.456, 0.406]), 'stds': array([0.229, 0.224, 0.225])}, {'name': 'lib.data.augmentations.augmentations.ConvertToTensor'}], 'image_keys': [('image', 0)], 'calib_keys': ['P2']}}\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict={}\n",
    "output_dict['cfg'] = train_dataset.__str__()\n",
    "output_dict['samples'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 22.483 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 5.1 seconds.\n",
      "======\n",
      "Found 850 in the v1.0-trainval\n"
     ]
    }
   ],
   "source": [
    "dataset = build(**train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18703/18703 [00:02<00:00, 6560.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for i in tqdm.tqdm(range(len(dataset)), dynamic_ncols=True):\n",
    "    token_index       = i // len(dataset.cameras)\n",
    "    camera_type_index = i % len(dataset.cameras)\n",
    "    camera_type       = dataset.cameras[camera_type_index]\n",
    "    \n",
    "    sample_tokens = dataset.token_list[token_index]\n",
    "    samples        = list(map(dataset.nusc_get_sample, sample_tokens))\n",
    "    camera_datas   = list(map(dataset.nusc_get_sample_data, [sample['data'][camera_type] for sample in samples]))\n",
    "    cs_records     = list(map(dataset.nusc_get_sensor, [camera_data['calibrated_sensor_token'] for camera_data in camera_datas]))\n",
    "    ego_records    = list(map(dataset.nusc_get_ego_pose, [camera_data['ego_pose_token'] for camera_data in camera_datas]))\n",
    "\n",
    "    image_dirs = [os.path.join(dataset.nuscenes_dir, camera_data['filename']) for camera_data in camera_datas]\n",
    "    P2 = dataset.get_intrinsic(cs_records[0])\n",
    "    extrinsics = list(map(dataset.get_extrinsic, cs_records)) #[T] 4 x 4 x 3\n",
    "    poses      = list(map(dataset.get_ego_pose, ego_records)) #[T] 4 x 4 x 3\n",
    "    relative_pose01 = cam_relative_pose_nusc(\n",
    "               poses[0], poses[1], np.linalg.inv(extrinsics[0])\n",
    "            ).astype(np.float32)\n",
    "    relative_pose02 = cam_relative_pose_nusc(\n",
    "               poses[0], poses[2], np.linalg.inv(extrinsics[0])\n",
    "            ).astype(np.float32)\n",
    "    translation01 = np.linalg.norm(relative_pose01[0:3, 3])\n",
    "    translation02 = np.linalg.norm(relative_pose02[0:3, 3])\n",
    "    is_static = False\n",
    "    if translation01 < dataset.filter_threshold or translation01 > 3:\n",
    "        is_static = True\n",
    "    if translation02 < dataset.filter_threshold or translation02 > 3:\n",
    "        is_static = True\n",
    "    if not is_static:\n",
    "        data = dict()\n",
    "        data['frame0'] = image_dirs[0]\n",
    "        data['frame1'] = image_dirs[1]\n",
    "        data['frame-1'] = image_dirs[2]\n",
    "        data['pose01'] = relative_pose01.reshape(-1).tolist()\n",
    "        data['pose0-1'] = relative_pose02.reshape(-1).tolist()\n",
    "        data['P2'] = P2.reshape(-1).tolist()\n",
    "        data['camera_type_indexes'] = camera_type_index\n",
    "        data['camera_type'] = camera_type\n",
    "        output_dict['samples'].append(data)\n",
    "print(len(output_dict['samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(output_dict, open('json_nusc_front_train.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = edict(\n",
    "    name = \"monodepth.data.datasets.nuscene_dataset.NusceneDepthMonoDataset\",\n",
    "    frame_idxs = [0, 1, -1],\n",
    "    is_motion_mask = False,\n",
    "    is_precompute_flow = False,\n",
    "    is_filter_static = True,\n",
    "    nuscenes_dir = NUSCPATH,\n",
    "    split_file = os.path.join(BASEPATH, 'meta_data', 'nusc_trainsub', 'nusc_val.txt'),   \n",
    "    channels = ['CAM_BACK', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK_LEFT', 'CAM_FRONT_LEFT'],\n",
    "    augmentation = edict(\n",
    "    name='vision_base.utils.builder.Sequential',\n",
    "    cfg_list=[\n",
    "        edict(name='vision_base.data.augmentations.augmentations.ConvertToFloat'),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.Resize', size=(288, 384), preserve_aspect_ratio=True, force_pad=True),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.Normalize', mean=np.array([0.485, 0.456, 0.406]), stds=np.array([0.229, 0.224, 0.225]),),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.ConvertToTensor'),\n",
    "    ],\n",
    "    image_keys=[('image', 0)], \n",
    "    calib_keys=['P2']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 850 in the v1.0-trainval\n"
     ]
    }
   ],
   "source": [
    "dataset = build(**val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25362/25362 [00:03<00:00, 7008.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25362\n"
     ]
    }
   ],
   "source": [
    "output_dict={}\n",
    "output_dict['cfg'] = val_dataset.__str__()\n",
    "output_dict['samples'] = []\n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(len(dataset)), dynamic_ncols=True):\n",
    "    token_index       = i // len(dataset.cameras)\n",
    "    camera_type_index = i % len(dataset.cameras)\n",
    "    camera_type       = dataset.cameras[camera_type_index]\n",
    "    \n",
    "    sample_tokens = dataset.token_list[token_index]\n",
    "    samples        = list(map(dataset.nusc_get_sample, sample_tokens))\n",
    "    camera_datas   = list(map(dataset.nusc_get_sample_data, [sample['data'][camera_type] for sample in samples]))\n",
    "    cs_records     = list(map(dataset.nusc_get_sensor, [camera_data['calibrated_sensor_token'] for camera_data in camera_datas]))\n",
    "    ego_records    = list(map(dataset.nusc_get_ego_pose, [camera_data['ego_pose_token'] for camera_data in camera_datas]))\n",
    "\n",
    "    image_dirs = [os.path.join(dataset.nuscenes_dir, camera_data['filename']) for camera_data in camera_datas]\n",
    "    P2 = dataset.get_intrinsic(cs_records[0])\n",
    "    extrinsics = list(map(dataset.get_extrinsic, cs_records)) #[T] 4 x 4 x 3\n",
    "    poses      = list(map(dataset.get_ego_pose, ego_records)) #[T] 4 x 4 x 3\n",
    "    relative_pose01 = cam_relative_pose_nusc(\n",
    "               poses[0], poses[1], np.linalg.inv(extrinsics[0])\n",
    "            ).astype(np.float32)\n",
    "    relative_pose02 = cam_relative_pose_nusc(\n",
    "               poses[0], poses[2], np.linalg.inv(extrinsics[0])\n",
    "            ).astype(np.float32)\n",
    "    data = dict()\n",
    "    data['frame0'] = image_dirs[0]\n",
    "    data['frame1'] = image_dirs[1]\n",
    "    data['frame-1'] = image_dirs[2]\n",
    "    data['pose01'] = relative_pose01.reshape(-1).tolist()\n",
    "    data['pose0-1'] = relative_pose02.reshape(-1).tolist()\n",
    "    data['P2'] = P2.reshape(-1).tolist()\n",
    "    data['camera_type_indexes'] = camera_type_index\n",
    "    data['camera_type'] = camera_type\n",
    "    output_dict['samples'].append(data)\n",
    "print(len(output_dict['samples']))\n",
    "json.dump(output_dict, open('json_nusc_all_val.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sweep_dataset = edict(\n",
    "    name = \"monodepth.data.datasets.nuscene_dataset.NusceneSweepDepthMonoDataset\",\n",
    "    frame_idxs = [0, 1, -1],\n",
    "    is_motion_mask = False,\n",
    "    is_precompute_flow = False,\n",
    "    is_filter_static = True,\n",
    "    nuscenes_dir = NUSCPATH,\n",
    "    split_file = os.path.join(BASEPATH, 'meta_data', 'nusc_trainsub', 'nusc_train.txt'),   \n",
    "    channels = ['CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT', 'CAM_BACK_LEFT', 'CAM_FRONT_LEFT'],\n",
    "    augmentation = edict(\n",
    "    name='vision_base.utils.builder.Sequential',\n",
    "    cfg_list=[\n",
    "        edict(name='vision_base.data.augmentations.augmentations.ConvertToFloat'),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.Resize', size=(288, 384), preserve_aspect_ratio=True, force_pad=True),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.Normalize', mean=np.array([0.485, 0.456, 0.406]), stds=np.array([0.229, 0.224, 0.225]),),\n",
    "        edict(name='vision_base.data.augmentations.augmentations.ConvertToTensor'),\n",
    "    ],\n",
    "    image_keys=[('image', 0)], \n",
    "    calib_keys=['P2']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 850 in the v1.0-trainval\n"
     ]
    }
   ],
   "source": [
    "dataset = build(**train_sweep_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74812/74812 [00:11<00:00, 6622.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60411\n"
     ]
    }
   ],
   "source": [
    "output_dict={}\n",
    "output_dict['cfg'] = train_sweep_dataset.__str__()\n",
    "output_dict['samples'] = []\n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(len(dataset)), dynamic_ncols=True):\n",
    "    token_index       = i // len(dataset.cameras)\n",
    "    camera_type_index = i % len(dataset.cameras)\n",
    "    camera_type       = dataset.cameras[camera_type_index]\n",
    "    \n",
    "    sample_tokens = dataset.token_list[token_index]\n",
    "    main_token     = sample_tokens[0] # center sample data\n",
    "    main_sample    = dataset.nusc_get_sample(main_token)\n",
    "    main_camera_instance = dataset.nusc_get_sample_data(main_sample['data'][camera_type])\n",
    "    camera_datas = [main_camera_instance]\n",
    "\n",
    "    for frame_id in dataset.frame_ids[1:]:\n",
    "        next_key = 'next' if frame_id > 0 else 'prev'\n",
    "        tmp_camera_instance = main_camera_instance\n",
    "        for _ in range(abs(frame_id)):\n",
    "            tmp_camera_instance = dataset.nusc_get_sample_data(tmp_camera_instance[next_key])\n",
    "        camera_datas.append(tmp_camera_instance)\n",
    "    cs_records     = list(map(dataset.nusc_get_sensor, [camera_data['calibrated_sensor_token'] for camera_data in camera_datas]))\n",
    "    ego_records    = list(map(dataset.nusc_get_ego_pose, [camera_data['ego_pose_token'] for camera_data in camera_datas]))\n",
    "\n",
    "    image_dirs = [os.path.join(dataset.nuscenes_dir, camera_data['filename']) for camera_data in camera_datas]\n",
    "    P2 = dataset.get_intrinsic(cs_records[0])\n",
    "    extrinsics = list(map(dataset.get_extrinsic, cs_records)) #[T] 4 x 4 x 3\n",
    "    poses      = list(map(dataset.get_ego_pose, ego_records)) #[T] 4 x 4 x 3\n",
    "    relative_pose01 = cam_relative_pose_nusc(\n",
    "               poses[0], poses[1], np.linalg.inv(extrinsics[0])\n",
    "            ).astype(np.float32)\n",
    "    relative_pose02 = cam_relative_pose_nusc(\n",
    "               poses[0], poses[2], np.linalg.inv(extrinsics[0])\n",
    "            ).astype(np.float32)\n",
    "    translation01 = np.linalg.norm(relative_pose01[0:3, 3])\n",
    "    translation02 = np.linalg.norm(relative_pose02[0:3, 3])\n",
    "    is_static = False\n",
    "    if translation01 < dataset.filter_threshold or translation01 > 3:\n",
    "        is_static = True\n",
    "    if translation02 < dataset.filter_threshold or translation02 > 3:\n",
    "        is_static = True\n",
    "    if not is_static:\n",
    "        data = dict()\n",
    "        data['frame0'] = image_dirs[0]\n",
    "        data['frame1'] = image_dirs[1]\n",
    "        data['frame-1'] = image_dirs[2]\n",
    "        data['pose01'] = relative_pose01.reshape(-1).tolist()\n",
    "        data['pose0-1'] = relative_pose02.reshape(-1).tolist()\n",
    "        data['P2'] = P2.reshape(-1).tolist()\n",
    "        data['camera_type_indexes'] = camera_type_index\n",
    "        data['camera_type'] = camera_type\n",
    "        output_dict['samples'].append(data)\n",
    "print(len(output_dict['samples']))\n",
    "json.dump(output_dict, open('json_nusc_sweep_train.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
